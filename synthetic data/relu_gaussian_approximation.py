# -*- coding: utf-8 -*-
"""ReLU gaussian approximation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq_EYHtBZLfpnqEtNtuAxiYizF8vKzST
"""

!pip install tikzplotlib

import jax.numpy as jnp
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from mpl_toolkits import mplot3d
import math
import time
import statistics
import random
import matplotlib
from scipy.linalg import orth
from google.colab import files
from google.colab import drive
import os
import seaborn as sns
import tikzplotlib

drive.mount('/content/drive',force_remount=True)

# from numba import jit,njit
# import seaborn as sns
# import tikzplotlib

np.random.seed(2021)

np.__version__

# We first generate data on the grid. 
def get_grid(num_x,num_y,minrange,maxrange):
    xx, yy = np.meshgrid(np.linspace(minrange,maxrange,num_x),np.linspace(minrange,maxrange,num_y))
    coords=np.c_[xx.ravel(), yy.ravel()]
    return coords

def plot_fn_heatmap(Worig,bias,var):
    y,x = np.meshgrid(np.linspace(-1,1,500),np.linspace(-1,1,500))
    orig_fn = Worig[0]*x + Worig[1]*y
    orig_fn = orig_fn + bias
    orig_fn = np.where(orig_fn<0,0,orig_fn)
    print("shape of orig fun:",orig_fn.shape)
    f_min,f_max = orig_fn.min(), orig_fn.max()
    fig,ax = plt.subplots()
    print("shape of x in heatmap:",x.shape,y.shape,orig_fn.shape)
    c = ax.pcolormesh(x,y,orig_fn,cmap='RdBu',vmin=f_min,vmax = f_max)
    ax.set_title('Original function, W=['+str(Worig[0])+','+str(Worig[1])+'], bias='+str(borig))
    ax.axis([x.min(),x.max(),y.min(),y.max()])
    fig.colorbar(c,ax=ax)
    fig = plt.gcf()
    plt.show()
    fig.savefig('orig.png')
    

def latex_plot_error(sample_sizes,error_l,error_u,Worig,borig,style,name):
    plt.figure(figsize=(10,6),tight_layout=True)
    plt.style.use(style)
    # plt.axhline(0, color='black')
    # plt.axvline(0, color='black')
    # plt.set_ylim(ymin=0)
    plt.grid('on')
    plt.xlabel('Sample Sizes')
    plt.ylabel('Relative Error',rotation=0)
    plt.title('Error of Approximation')
    plt.plot(sample_sizes,error_l,'--',label="Leverage Score Measure")
    plt.plot(sample_sizes,error_u,':',label="Gaussian measure")
    plt.legend(loc='upper right', shadow=True)
    tikzplotlib.save(name+'.tex')

print('matplotlib: {}'.format(matplotlib.__version__))

def apply_relu(X,W,b,noise):
    Y = np.matmul(W,X.T) + b
    Y = np.where(Y<0,0,Y)
    return Y+noise

def apply_abs(X,W,b,noise):
    Y = np.abs(np.matmul(W,X.T) + b)
    return Y + noise
    
    
def get_leverage_scores(grid):
    U = orth(grid)
#     print("Size of basis:",grid.shape)
    lev_scores = np.linalg.norm(U, axis=1)**2
#     print("Size of leverage scores",lev_scores.shape)
    return lev_scores

def get_minimiser(X,Y,setofW,biases,isleverage,lev_scores):
    Yhat = np.matmul(X,setofW.T)
    Yhat = np.expand_dims(Yhat,axis=2)
    Yhat = Yhat + biases
#     print("shape of Yhat",Yhat.shape)
    Yhat = np.where(Yhat<0,0,Yhat)
    Y = np.expand_dims(Y,axis=(1,2))
    loss = (Yhat - Y)**2
#     print("shape of Y,Yhat", Y.shape,Yhat.shape)
#     print("shape of loss:",loss.shape)
    #We multiply by the leverage scores if the minimiser is leverage
    if(isleverage == True):
        lev_scores = np.expand_dims(lev_scores,axis=(1,2))
        loss = loss*lev_scores/np.sum(lev_scores)
    loss = np.sum(loss,axis=0)
    ind = np.unravel_index(loss.argmin(), loss.shape)
    
    return (setofW[ind[0]],biases[ind[1]])

def get_error(Worig,borig,W_hat,b_hat):
    X = get_grid(101,101,-1,1)
    b_hat = np.expand_dims(b_hat,axis=2)
    f_orig = apply_relu(X,Worig,borig,0)
    f_hat = apply_relu(X,W_hat,b_hat,0)
    
    print("Shape of f:",f_orig.shape)
    error = np.sum(np.square(np.abs(f_hat - f_orig))/(np.linalg.norm(f_orig)**2),axis = 2)
    error_med = np.median(error,axis = 1)
    error_mean = np.mean(error,axis = 1)
    return (error_med,error_mean)

def plot_error(sample_sizes,error_l,error_u,Worig,borig,mean=False):
    fig,ax = plt.subplots()
    plt.xticks(np.log(sample_sizes))
    plt.plot(np.log(sample_sizes),error_l,label="Leverage Score Measure",color="green")
    plt.plot(np.log(sample_sizes),error_u,label="Uniform measure",color="blue")
    plt.title("Relative error")
    plt.xlabel('Number of Samples taken for learning')
    plt.ylabel('Relative error')
    plt.grid(True)
    # plt.style.use('_mpl-gallery')
    ax.set_ylim(ymin=0)
    plt.legend()
#     plt.show()
    if(mean):
      plt.savefig('Error_mean.png')
    else:
      plt.savefig('Error_median.png')
    # files.download('Error.png')

def fit_relu(sample_sizes,Worig,borig,noise_variance,biases,d=2,num_iterations=100):
    #We get the leverage scores of the grid
    ones_column = np.ones(1000)
    mean = (0,0)
    cov = [[1, 0], [0,1]]

    grid = get_grid(101,101,-1,1)
    gaussian_grid = np.random.multivariate_normal(mean,cov, size = 1000)
    gaussian_grid = np.insert(gaussian_grid,2,ones_column,axis=1)
    print("grid size:",grid.shape)
    lev_scores = get_leverage_scores(gaussian_grid)
    
    #Creating empty arrays to store the minimisers returned for each sample size, for each iteration
    W_hat_uniform = np.zeros((len(sample_sizes),num_iterations,d))
    b_hat_uniform = np.zeros((len(sample_sizes),num_iterations))
    
    W_hat_leverage = np.zeros((len(sample_sizes),num_iterations,d))
    b_hat_leverage = np.zeros((len(sample_sizes),num_iterations))
    j = 0
    for m in sample_sizes:
        print("calculating for m=",m)
        #Creating empty arrays to store minimisers for each iteration 
        W_it_u = np.zeros((num_iterations,d))
        b_it_u = np.zeros((num_iterations))
        
        W_it_l = np.zeros((num_iterations,d))
        b_it_l = np.zeros((num_iterations))
        
        for itr in range(num_iterations):
            noise = np.random.normal(0,noise_variance,m)
            # Xu = np.random.multivariate_normal(mean,cov, size = m)
            Xu = gaussian_grid[np.random.choice(gaussian_grid.shape[0], size=m, replace=False),:-1]
            Yu = apply_relu(Xu,Worig,borig,noise)
            #Sample data using leverage scores
        
            #We first calculate the leverage scores of the grid
            indices = np.random.choice(gaussian_grid.shape[0],size=m,replace=False,p=lev_scores/np.sum(lev_scores))
            Xl = gaussian_grid[indices,:-1]
            # print('Lets see Xl',Xl)
            # print('Lets see grid',grid[indices])
            sampled_lev_scores = lev_scores[indices]
            Yl = apply_relu(Xl,Worig,borig,noise)
            
            #Let's see the points
            if(itr==20):
                plt.scatter(Xu[:,0],Xu[:,1],c='b',label='Uniform')
                plt.scatter(Xl[:,0], Xl[:,1], c='g',label='Leverage')
                plt.show()
            
            # Solving using uniform sampling
            Wmin_u,bmin_u = get_minimiser(Xu,Yu,grid,biases,isleverage=False,lev_scores=sampled_lev_scores)
            W_it_u[itr,:]= Wmin_u
            b_it_u[itr] = bmin_u
            
            #Solving using leverage score sampling
            Wmin_l,bmin_l = get_minimiser(Xl,Yl,grid,biases,isleverage=True,lev_scores=sampled_lev_scores)
            W_it_l[itr,:] = Wmin_l
            b_it_l[itr] = bmin_l
            
        W_hat_uniform[j,:] = W_it_u
        W_hat_leverage[j,:] = W_it_l
        
        b_hat_uniform[j,:] = b_it_u
        b_hat_leverage[j,:] = b_it_l
        j = j+1
    return (W_hat_uniform,W_hat_leverage,b_hat_uniform,b_hat_leverage)

# creating a 2D grid
X = get_grid(101,101,-1,1)
# setting the original weight
Worig = np.array([0.4,0.4])
borig = -0.4

setofW = get_grid(101,101,-1,1)
biases = np.linspace(-1,1,50)

var = 0.05
# We now visualize the function we generated
plot_fn_heatmap(Worig,borig,var)
sample_sizes = [10,20,30,35,40,60,80,100]

#We try to fit relu using uniform sampling and leverage score sampling
W_hat_uniform,W_hat_leverage,b_hat_uniform,b_hat_leverage = fit_relu(sample_sizes,Worig,borig,noise_variance = var,d=2,biases = biases, num_iterations=100)

#Let's see how well our optimised Ws perform
error_u_med,error_u_mean = get_error(Worig,borig,W_hat_uniform,b_hat_uniform)
error_l_med,error_l_mean = get_error(Worig,borig,W_hat_leverage,b_hat_leverage)

#Final plots
plot_error(sample_sizes,error_l_med,error_u_med,Worig,borig,mean=False)
plot_error(sample_sizes,error_l_mean,error_u_mean, Worig,borig,mean=True)

np.save('error_l_med.npy',error_l_med)
np.save('error_l_mean.npy',error_l_mean)
np.save('error_u_med.npy',error_u_med)
np.save('error_u_mean.npy',error_u_mean)
np.save('sample_sizes.npy',sample_sizes)

#We now copy the data onto google drive
drive.mount('/content/drive', force_remount=True)

path = "drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats_1_var_0.1"
os.mkdir(path)
# !cp error_l_med.npy /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/
# !cp error_l_mean.npy /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/
# !cp error_u_med.npy /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/
# !cp error_u_mean.npy /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/

!cp orig.png /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats_1_var_0.1/
# !cp Error_mean.png /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/
# !cp Error_median.png /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/
# !cp sample_sizes.npy /content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats/

plt.rcParams.update(plt.rcParamsDefault)
style = 'seaborn-dark-palette'
latex_plot_error(sample_sizes,error_l_med,error_u_med,Worig,borig,style,'med_plot')
!cp med_plot.tex/content/drive/MyDrive/ICLR_2023/relu_gaussian/exp_aistats_1_var_0.1/

plt.style.available



arr = np.array([1,1,1,1])

Worig = [0.4,0.4]
str(Worig[0])

